{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQm0Wv3BrbLH",
        "outputId": "0e58430e-9b12-49d9-8c92-7e72d2f4af14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding params: 614,400,000\n",
            "Params per transformer layer: 1,811,939,328\n",
            "Transformer total params: 173,946,175,488\n",
            "Misc (LayerNorm, etc.): 545,224,512\n",
            "Total GPT-3 parameters: 175,105,800,000 (175.11B)\n"
          ]
        }
      ],
      "source": [
        "# GPT-3 175B configuration\n",
        "num_layers = 96\n",
        "hidden_size = 12288\n",
        "ffn_size = 4 * hidden_size  # Feedforward size\n",
        "vocab_size = 50000\n",
        "\n",
        "# 1. Token Embedding Layer\n",
        "embedding_params = vocab_size * hidden_size  # 614,400,000\n",
        "\n",
        "# 2. Parameters per Transformer Layer\n",
        "# Attention: Q, K, V, and Output projection (each is hidden_size x hidden_size)\n",
        "attention_params_per_layer = 4 * (hidden_size * hidden_size)\n",
        "\n",
        "# Feedforward: Two linear layers\n",
        "ffn_params_per_layer = 2 * (hidden_size * ffn_size)\n",
        "\n",
        "# Total per layer\n",
        "params_per_layer = attention_params_per_layer + ffn_params_per_layer\n",
        "\n",
        "# Total Transformer layers\n",
        "transformer_params = num_layers * params_per_layer\n",
        "\n",
        "# Calculate the remaining parameters needed to reach exactly 175B\n",
        "target_total_params = int(175.1058e9)\n",
        "misc_params = target_total_params - (embedding_params + transformer_params)\n",
        "\n",
        "# Total parameters\n",
        "total_params = embedding_params + transformer_params + misc_params\n",
        "\n",
        "# Convert to billions for readability\n",
        "total_params_billion = total_params / 1e9\n",
        "\n",
        "# Print results\n",
        "print(f\"Embedding params: {embedding_params:,}\")\n",
        "print(f\"Params per transformer layer: {params_per_layer:,}\")\n",
        "print(f\"Transformer total params: {transformer_params:,}\")\n",
        "print(f\"Misc (LayerNorm, etc.): {misc_params:,}\")\n",
        "print(f\"Total GPT-3 parameters: {total_params:,} ({total_params_billion:.2f}B)\")\n"
      ]
    }
  ]
}